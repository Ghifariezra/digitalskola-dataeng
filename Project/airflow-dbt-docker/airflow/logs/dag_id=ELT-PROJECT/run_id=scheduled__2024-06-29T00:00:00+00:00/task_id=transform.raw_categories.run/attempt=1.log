[2024-07-14T04:03:17.586+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-14T04:03:17.701+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ELT-PROJECT.transform.raw_categories.run scheduled__2024-06-29T00:00:00+00:00 [queued]>
[2024-07-14T04:03:17.772+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ELT-PROJECT.transform.raw_categories.run scheduled__2024-06-29T00:00:00+00:00 [queued]>
[2024-07-14T04:03:17.775+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-07-14T04:03:17.858+0000] {taskinstance.py:2330} INFO - Executing <Task(DbtRunLocalOperator): transform.raw_categories.run> on 2024-06-29 00:00:00+00:00
[2024-07-14T04:03:17.897+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'ELT-PROJECT', 'transform.raw_categories.run', 'scheduled__2024-06-29T00:00:00+00:00', '--job-id', '1042', '--raw', '--subdir', 'DAGS_FOLDER/elt.py', '--cfg-path', '/tmp/tmpn266bz2x']
[2024-07-14T04:03:17.904+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=12385) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-07-14T04:03:17.914+0000] {standard_task_runner.py:63} INFO - Started process 12419 to run task
[2024-07-14T04:03:17.913+0000] {standard_task_runner.py:91} INFO - Job 1042: Subtask transform.raw_categories.run
[2024-07-14T04:03:18.032+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-07-14T04:03:18.270+0000] {task_command.py:426} INFO - Running <TaskInstance: ELT-PROJECT.transform.raw_categories.run scheduled__2024-06-29T00:00:00+00:00 [running]> on host airflow-scheduler
[2024-07-14T04:03:18.845+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Data-Ninja' AIRFLOW_CTX_DAG_ID='ELT-PROJECT' AIRFLOW_CTX_TASK_ID='transform.raw_categories.run' AIRFLOW_CTX_EXECUTION_DATE='2024-06-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-29T00:00:00+00:00'
[2024-07-14T04:03:18.852+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-14T04:03:18.876+0000] {local.py:174} INFO - Could not import dbtRunner. Falling back to subprocess for invoking dbt.
[2024-07-14T04:03:18.879+0000] {local.py:292} INFO - Cloning project to writable temp directory /tmp/tmp36h0f2hx from /opt/airflow/dags/dbt/dbt_project
[2024-07-14T04:03:18.896+0000] {local.py:303} INFO - Partial parse is enabled and the latest partial parse file is /tmp/cosmos/ELT-PROJECT__transform/target/partial_parse.msgpack
[2024-07-14T04:03:19.040+0000] {base.py:84} INFO - Using connection ID 'snow_conn' for task execution.
[2024-07-14T04:03:19.049+0000] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.
[2024-07-14T04:03:19.075+0000] {config.py:298} INFO - Profile caching is enable.
[2024-07-14T04:03:19.094+0000] {config.py:272} INFO - Profile found in cache using profile: /tmp/cosmos/profile/a0017dab0633177292578162811b91dcb9c56d6e54017e9660056cc364a762ad/profiles.yml.
[2024-07-14T04:03:19.098+0000] {local.py:247} INFO - Trying to run the command:
 ['/opt/airflow/dbt_venv/bin/dbt', 'deps', '--project-dir', '/tmp/tmp36h0f2hx', '--profiles-dir', '/tmp/cosmos/profile/a0017dab0633177292578162811b91dcb9c56d6e54017e9660056cc364a762ad', '--profile', 'dbt_project', '--target', 'dev']
From /tmp/tmp36h0f2hx
[2024-07-14T04:03:19.105+0000] {subprocess.py:60} INFO - Tmp dir root location: 
 /tmp
[2024-07-14T04:03:19.109+0000] {subprocess.py:73} INFO - Running command: ['/opt/airflow/dbt_venv/bin/dbt', 'deps', '--project-dir', '/tmp/tmp36h0f2hx', '--profiles-dir', '/tmp/cosmos/profile/a0017dab0633177292578162811b91dcb9c56d6e54017e9660056cc364a762ad', '--profile', 'dbt_project', '--target', 'dev']
[2024-07-14T04:03:19.196+0000] {subprocess.py:84} INFO - Command output:
[2024-07-14T04:03:36.020+0000] {subprocess.py:94} INFO - [0m04:03:35  Running with dbt=1.8.3
[2024-07-14T04:03:37.816+0000] {subprocess.py:94} INFO - [0m04:03:37  Updating lock file in file path: /tmp/tmp36h0f2hx/package-lock.yml
[2024-07-14T04:03:37.889+0000] {subprocess.py:94} INFO - [0m04:03:37  Installing dbt-labs/dbt_utils
[2024-07-14T04:03:38.942+0000] {subprocess.py:94} INFO - [0m04:03:38  Installed from version 0.8.6
[2024-07-14T04:03:38.955+0000] {subprocess.py:94} INFO - [0m04:03:38  Updated version available: 1.2.0
[2024-07-14T04:03:38.963+0000] {subprocess.py:94} INFO - [0m04:03:38
[2024-07-14T04:03:38.972+0000] {subprocess.py:94} INFO - [0m04:03:38  Updates available for packages: ['dbt-labs/dbt_utils']
[2024-07-14T04:03:38.974+0000] {subprocess.py:94} INFO - Update your versions in packages.yml, then run dbt deps
[2024-07-14T04:03:41.789+0000] {subprocess.py:98} INFO - Command exited with return code 0
[2024-07-14T04:03:41.792+0000] {local.py:254} INFO - Update your versions in packages.yml, then run dbt deps
[2024-07-14T04:03:41.795+0000] {local.py:247} INFO - Trying to run the command:
 ['/opt/airflow/dbt_venv/bin/dbt', 'run', '--models', 'raw_categories', '--project-dir', '/tmp/tmp36h0f2hx', '--profiles-dir', '/tmp/cosmos/profile/a0017dab0633177292578162811b91dcb9c56d6e54017e9660056cc364a762ad', '--profile', 'dbt_project', '--target', 'dev']
From /tmp/tmp36h0f2hx
[2024-07-14T04:03:41.803+0000] {subprocess.py:60} INFO - Tmp dir root location: 
 /tmp
[2024-07-14T04:03:41.805+0000] {subprocess.py:73} INFO - Running command: ['/opt/airflow/dbt_venv/bin/dbt', 'run', '--models', 'raw_categories', '--project-dir', '/tmp/tmp36h0f2hx', '--profiles-dir', '/tmp/cosmos/profile/a0017dab0633177292578162811b91dcb9c56d6e54017e9660056cc364a762ad', '--profile', 'dbt_project', '--target', 'dev']
[2024-07-14T04:03:41.885+0000] {subprocess.py:84} INFO - Command output:
[2024-07-14T04:03:57.779+0000] {subprocess.py:94} INFO - [0m04:03:57  Running with dbt=1.8.3
[2024-07-14T04:04:01.377+0000] {subprocess.py:94} INFO - [0m04:04:01  Registered adapter: snowflake=1.8.3
[2024-07-14T04:04:06.986+0000] {subprocess.py:94} INFO - [0m04:04:06  Found 20 models, 27 data tests, 11 sources, 684 macros
[2024-07-14T04:04:07.011+0000] {subprocess.py:94} INFO - [0m04:04:07
[2024-07-14T04:04:15.969+0000] {subprocess.py:94} INFO - [0m04:04:15  Concurrency: 1 threads (target='dev')
[2024-07-14T04:04:15.971+0000] {subprocess.py:94} INFO - [0m04:04:15
[2024-07-14T04:04:16.022+0000] {subprocess.py:94} INFO - [0m04:04:16  1 of 1 START sql table model raw.raw_categories ................................ [RUN]
[2024-07-14T04:04:19.517+0000] {subprocess.py:94} INFO - [0m04:04:19  1 of 1 OK created sql table model raw.raw_categories ........................... [[32mSUCCESS 1[0m in 3.48s]
[2024-07-14T04:04:19.529+0000] {subprocess.py:94} INFO - [0m04:04:19
[2024-07-14T04:04:19.532+0000] {subprocess.py:94} INFO - [0m04:04:19  Finished running 1 table model in 0 hours 0 minutes and 12.52 seconds (12.52s).
[2024-07-14T04:04:19.878+0000] {subprocess.py:94} INFO - [0m04:04:19
[2024-07-14T04:04:19.881+0000] {subprocess.py:94} INFO - [0m04:04:19  [32mCompleted successfully[0m
[2024-07-14T04:04:19.889+0000] {subprocess.py:94} INFO - [0m04:04:19
[2024-07-14T04:04:19.892+0000] {subprocess.py:94} INFO - [0m04:04:19  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[2024-07-14T04:04:22.237+0000] {subprocess.py:98} INFO - Command exited with return code 0
[2024-07-14T04:04:22.247+0000] {local.py:254} INFO - [0m04:04:19  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[2024-07-14T04:04:22.938+0000] {local.py:196} WARNING - Artifact schema version: https://schemas.getdbt.com/dbt/manifest/v12.json is above dbt-ol supported version 7. This might cause errors.
[2024-07-14T04:04:22.940+0000] {local.py:196} WARNING - Artifact schema version: https://schemas.getdbt.com/dbt/run-results/v6.json is above dbt-ol supported version 5. This might cause errors.
[2024-07-14T04:04:22.958+0000] {local.py:349} INFO - Inlets: [Dataset(uri='snowflake://OU00370.europe-west2.gcp/ELT_BATCH.PUBLIC.categories', extra=None)]
[2024-07-14T04:04:22.965+0000] {local.py:350} INFO - Outlets: [Dataset(uri='snowflake://OU00370.europe-west2.gcp/ELT_BATCH.raw.raw_categories', extra=None)]
[2024-07-14T04:04:22.987+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-14T04:04:23.339+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/datasets/manager.py:53 SAWarning: DELETE statement on table 'task_outlet_dataset_reference' expected to delete 1 row(s); 0 were matched.  Please set confirm_deleted_rows=False within the mapper configuration to prevent this warning.
[2024-07-14T04:04:23.441+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-14T04:04:23.447+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "todr_pkey"
DETAIL:  Key (dataset_id, dag_id, task_id)=(2, ELT-PROJECT, transform.raw_categories.run) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/base.py", line 270, in execute
    self.build_and_run_cmd(context=context, cmd_flags=self.add_cmd_flags())
  File "/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py", line 489, in build_and_run_cmd
    result = self.run_command(cmd=dbt_cmd, env=env, context=context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py", line 351, in run_command
    self.register_dataset(inlets, outlets)
  File "/home/airflow/.local/lib/python3.12/site-packages/cosmos/operators/local.py", line 443, in register_dataset
    DAG.bulk_write_to_db([self.dag], session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3295, in bulk_write_to_db
    session.bulk_save_objects(task_refs_to_add)
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3705, in bulk_save_objects
    self._bulk_save_mappings(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3912, in _bulk_save_mappings
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 3901, in _bulk_save_mappings
    persistence._bulk_insert(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 107, in _bulk_insert
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "todr_pkey"
DETAIL:  Key (dataset_id, dag_id, task_id)=(2, ELT-PROJECT, transform.raw_categories.run) already exists.

[SQL: INSERT INTO task_outlet_dataset_reference (dataset_id, dag_id, task_id, created_at, updated_at) VALUES (%(dataset_id)s, %(dag_id)s, %(task_id)s, %(created_at)s, %(updated_at)s)]
[parameters: {'dataset_id': 2, 'dag_id': 'ELT-PROJECT', 'task_id': 'transform.raw_categories.run', 'created_at': datetime.datetime(2024, 7, 14, 4, 4, 23, 361602, tzinfo=Timezone('UTC')), 'updated_at': datetime.datetime(2024, 7, 14, 4, 4, 23, 361628, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-07-14T04:04:23.604+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=ELT-PROJECT, task_id=transform.raw_categories.run, run_id=scheduled__2024-06-29T00:00:00+00:00, execution_date=20240629T000000, start_date=20240714T040317, end_date=20240714T040423
[2024-07-14T04:04:23.642+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 1042 for task transform.raw_categories.run ((psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "todr_pkey"
DETAIL:  Key (dataset_id, dag_id, task_id)=(2, ELT-PROJECT, transform.raw_categories.run) already exists.

[SQL: INSERT INTO task_outlet_dataset_reference (dataset_id, dag_id, task_id, created_at, updated_at) VALUES (%(dataset_id)s, %(dag_id)s, %(task_id)s, %(created_at)s, %(updated_at)s)]
[parameters: {'dataset_id': 2, 'dag_id': 'ELT-PROJECT', 'task_id': 'transform.raw_categories.run', 'created_at': datetime.datetime(2024, 7, 14, 4, 4, 23, 361602, tzinfo=Timezone('UTC')), 'updated_at': datetime.datetime(2024, 7, 14, 4, 4, 23, 361628, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 12419)
[2024-07-14T04:04:23.696+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-07-14T04:04:23.743+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
